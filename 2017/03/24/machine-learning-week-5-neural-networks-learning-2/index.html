<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Machine learning week 5 - Neural Networks: Learning</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="../../../../favicon.ico">

    <link rel="stylesheet" type="text/css" href="../../../../assets/css/screen.css?v=0c292fd43d">
    <link rel="stylesheet" type="text/css" href="../../../../assets/css/webkid.css?v=0c292fd43d">
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400">

    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Dong Liang's Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Machine learning week 5 - Neural Networks: Learning">
    <meta property="og:description" content="Cost Function Neural network model Let's first define a few variables that we will need to use: L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes Cost function Note: the double sum simply adds">
    <meta property="og:url" content="http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/">
    <meta property="article:published_time" content="2017-03-24T07:41:30.000Z">
    <meta property="article:modified_time" content="2017-04-13T02:49:54.000Z">
    <meta property="article:tag" content="machine learning">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Machine learning week 5 - Neural Networks: Learning">
    <meta name="twitter:description" content="Cost Function Neural network model Let's first define a few variables that we will need to use: L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes Cost function Note: the double sum simply adds">
    <meta name="twitter:url" content="http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Dong Liang">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="machine learning">
    <meta name="twitter:site" content="@ldifs">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Dong Liang&#x27;s Blog",
        "logo": "http://localhost:2368/ghost/img/ghosticon.jpg"
    },
    "author": {
        "@type": "Person",
        "name": "Dong Liang",
        "url": "http://localhost:2368/author/dong/",
        "sameAs": []
    },
    "headline": "Machine learning week 5 - Neural Networks: Learning",
    "url": "http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/",
    "datePublished": "2017-03-24T07:41:30.000Z",
    "dateModified": "2017-04-13T02:49:54.000Z",
    "keywords": "machine learning",
    "description": "Cost Function Neural network model Let&#x27;s first define a few variables that we will need to use: L &#x3D; total number of layers in the network sl &#x3D; number of units (not counting bias unit) in layer l K &#x3D; number of output units/classes Cost function Note: the double sum simply adds",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368"
    }
}
    </script>

    <script type="text/javascript" src="../../../../shared/ghost-url.js?v=0c292fd43d"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "25d5f7346569"
});
</script>
    <meta name="generator" content="Ghost 0.11">
    <link rel="alternate" type="application/rss+xml" title="Dong Liang's Blog" href="../../../../rss.4">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/github.min.css">
<script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head>
<body class="post-template tag-machine-learning">

    

<link rel="stylesheet" href="../../../../assets/css/prism.css">
<script src="../../../../assets/js/prism.js"></script>

<header id="header">
    <nav class="header-nav" style="background-image:url(../../../../content/images/2017/04/84705062-4C1F-4C1B-B60D-BEC8281F9CC7-1.JPG)">

      <div class="blog-title"><a href="../../../../">Dong Liang's Blog</a></div>

      <!--Change this to customize the navigation-->
      <div class="taglist-wrapper clearfix">
        <ul id="taglist" class="taglist">
    <li class="nav-home" role="presentation">
      <a href="../../../../">Home</a>
    </li>
    <li class="nav-machine-learning" role="presentation">
      <a href="../../../../tag/machine-learning/">Machine Learning</a>
    </li>
    <li class="nav-my-profile" role="presentation">
      <a href="http://localhost:2368/profile/">My profile</a>
    </li>
</ul>

      </div>
    </nav>
</header>


<main id="content" class="content" role="main">

    <div id="article" class="box ">
    
    <article class="post tag-machine-learning featured">

        <header class="post-header">
            <h1 class="post-title">Machine learning week 5 - Neural Networks: Learning</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2017-03-24">24 March 2017</time>
                <span class="author">by <a href="../../../../author/dong/">Dong Liang</a></span>
            </section>
        </header>

        <section class="post-content">
            <h3 id="costfunction">Cost Function</h3>

<p><strong>Neural network model</strong>
<img src="http://7d9rd6.com1.z0.glb.clouddn.com/wp-content/uploads/2016/04/scrn20160408215047.png" alt="neural network"></p>

<p>Let's first define a few variables that we will need to use:</p>

<ul>
<li>L = total number of layers in the network</li>
<li>sl = number of units (not counting bias unit) in layer l</li>
<li>K = number of output units/classes</li>
</ul>

<p><mark><strong>Cost function</strong></mark></p>

<p><img src="../../../../content/images/2017/03/Screen-Shot-2017-03-24-at-1.54.34-AM.png" alt="cost function"></p>

<p><strong><em>Note:</em></strong></p>

<ul>
<li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer</li>
<li>the triple sum simply adds up the squares of all the individual Θs in the entire network.</li>
<li>the i in the triple sum does not refer to training example i</li>
</ul>

<hr>

<h3 id="backpropagationalgorithm">Backpropagation Algorithm</h3>

<blockquote>
  <p>"Backpropagation" is neural-network terminology for minimizing our cost function. The goal of Backpropagation is to minimize J(theta) - the cost function of NN, using an optimal set of parameters in theta: namely minΘJ(Θ).</p>
</blockquote>

<p><strong>Backpropagation Intuition</strong></p>

<p><img src="../../../../content/images/2017/03/Screen-Shot-2017-03-31-at-3.35.53-PM.png" alt=""></p>

<p><em>*The δ(l)j is the error for a(l)j, expressed as the derivative of the cost function.The derivative is the slope of a line tangent to the cost fucntion, so the steeper the slope the more incorrect we are. *</em></p>

<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/qc309rdcEea4MxKdJPaTxA_324034f1a3c3a3be8e7c6cfca90d3445_fixx.png?expiry=1491091200000&amp;hmac=xP_kro6Z8DjXgkDS6Fldh99xn4GgBdcP0ffsNj7UUMs" alt=""></p>

<p>In the image above, to calculate δ(2)2, we multiply the weights Θ(2)12 and Θ(2)22 by their respective δ values found to the right of each edge. So we get δ(2)2= Θ(2)12<em>δ(3)1+Θ(2)22</em>δ(3)2. To calculate every single possible δ(l)j, we could start from the right of our diagram. We can think of our edges as our Θij. Going from right to left, to calculate the value of δ(l)j, you can just take the over all sum of each weight times the δ it is coming from. Hence, another example would be δ(3)2=Θ(3)12*δ(4)1.</p>

<hr>

<h3 id="unrollingparameters">Unrolling Parameters</h3>

<ul>
<li>It becomes more convenient when the theta or gradient parameters are packed into a bigger matrix, facilitating the vectorized operations. </li>
<li>Unrolling matrix of parameters and putting them into one long vector before using the optimizing function such as fminunc(). </li>
</ul>

<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/kdK7ubT2EeajLxLfjQiSjg_d35545b8d6b6940e8577b5a8d75c8657_Screenshot-2016-11-27-15.09.24.png?expiry=1491004800000&amp;hmac=qh2Gn1mRQ-khB5O2476dP0dkmXPypNn46ODoxmTmvHM" alt=""></p>

<hr>

<h3 id="gradientchecking">Gradient Checking</h3>

<p>Gradient Checking is designed to ensure that the deltaVector function works as intended. We can approximate the derivative of the cost function J(theta). </p>

<p>The approximation can be implemented by adding or subtracting epsilon to the theta matrix, as shown in octave codes as follows:</p>

<pre><code>epsilon = 1e-4;  
for i = 1:n,  
  thetaPlus = theta;
  thetaPlus(i) += epsilon;
  thetaMinus = theta;
  thetaMinus(i) -= epsilon;
  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)
end;  
</code></pre>

<hr>

<h3 id="randominitialization">Random Initialization</h3>

<p>The use of all-zero matrix for parameters initialization would result in repeated updating to the same values in all nodes during backpropagation. Random initialization of weights for theta matrix circumvent the problem of symmetry breaking.</p>

<p>Initializing weights with zeros for theta matrix doesn't work with neural network. When we backpropogate, the optimal values will update to the same values for all nodes. </p>

<p>Briefly, we initialize each theta with a random value between [-epsilon , epsilon]. Using this formula guarantees that we get the desired bound. We can apply the same technique to multi-dimentional J(theta) matrix. </p>

<pre><code>If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  
Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  
Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  
</code></pre>

<hr>

<h3 id="puttingittogether">Putting it Together</h3>

<p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.</p>

<ul>
<li>Number of input units = dimension of features x(i)</li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</li>
<li>Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.</li>
</ul>

<p><strong>Training a Neural Network</strong></p>

<ol>
<li>Randomly initialize the weights  </li>
<li>Implement forward propagation to get hΘ(x(i)) for any x(i)  </li>
<li>Implement the cost function  </li>
<li>Implement backpropagation to compute partial derivatives  </li>
<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.  </li>
<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</li>
</ol>

<pre><code>for i = 1:m,  
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
</code></pre>

<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/hGk18LsaEea7TQ6MHcgMPA_8de173808f362583eb39cdd0c89ef43e_Screen-Shot-2016-12-05-at-10.40.35-AM.png?expiry=1491091200000&amp;hmac=HbE_ZEpKeYD2TltrrYVU5OxTmXQd83nuso-WkozYsoM" alt=""></p>

<p>The image shows an intuition of how gradient descent works as we are implementing our neural network. The gradient descent algorithm keeps iterating and exploring the data pool until it reaches a local minimum where hΘ(x(i)) ~= y(i). The optimization process, as the the cost function is not convex, could only result in local minimum instead getting to the global optimum, but it normally doesn't cause huge problem in practice.</p>

<hr>

<h3 id="summarization">Summarization</h3>

<ul>
<li>Cost function - J(theta)</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-3.07.59-PM-1.png" alt=""></p>

<pre><code>J = (1/m) .* sum ( sum ((-yk) .* log(h_theta)  -  (1-yk) .* log(1-h_theta) )) + ...  
    lambda ./ (2 * m)  .* (sum(sum(Theta1(:, 2:end) .* Theta1(:, 2:end))) + ...
    sum(sum(Theta2(:, 2:end) .* Theta2(:, 2:end))))
</code></pre>

<ul>
<li>Forward propagation</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-7.00.04-PM.png" alt=""></p>

<ul>
<li>Backward propagation</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-3.20.34-PM-1.png" alt=""></p>

<p>The intuition behind the backward propagation can be described as: given dataset of training sample (Xt, Yt), we first calculate the activations of the whole neural network via forward propagation, including those for neural units in each hidden layer and of output value for h(theta). We then compute the error terms that reflect how much of that node was 'responsible' for any errors in our output. Concretely, for the output layer the error terms can be directly measured by subtracting actual value from network's activations. For the hidden units, the error terms in layer L can be computed based on weighted average of the error terms of the nodes in layer L + 1, that can be expressed as an equation that involves multiplication of matrix of theta, error terms of next layer and sigmoid gradient.</p>

<p><em>Sigmoid gradient</em></p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-3.15.56-PM-1.png" alt=""></p>

<p><em>Random initialization</em></p>

<pre><code>% Randomly initialize the weights to small values
% epsilon_init = sqrt(6)/(sqrt(L_in) + sqrt(L_out))
epsilon init = 0.12;  
W = rand(L out, 1 + L in) * 2 * epsilon init − epsilon init;  
</code></pre>

<ul>
<li>Backward propagation implementation</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.49.30-PM.png" alt=""></p>

<p>1) Use loop function over training samples and compute the activations for each layer, namely values of z and a. Note that an all-one term needs be added to the vectors of activations to include the bias unit in each layer. </p>

<p>2) Calculate the delta value(errors) for output layer by measuring the difference between the activation and true value. </p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.37.16-PM.png" alt=""></p>

<p>3) Calculate the error terms, namely the delta value, for hidden layer as follows:</p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.40.31-PM.png" alt=""></p>

<p>4) Compute the accumulated gradient for given sample using the following formulas, where the delta2_0 is skipped or removed. </p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.45.06-PM.png" alt=""></p>

<p>5) The gradient for the neural network cost function can then be obtained by dividing the accumulated gradients by 1/m:</p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.46.51-PM.png" alt=""></p>
        </section>

        <footer class="post-footer">



            <section class="author">
                <h4><a href="../../../../author/dong/">Dong Liang</a></h4>

                    <p>Read <a href="../../../../author/dong/">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="fa fa-twitter" href="https://twitter.com/share?text=Machine%20learning%20week%205%20-%20Neural%20Networks%3A%20Learning&amp;url=http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="fa fa-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="fa fa-google" href="https://plus.google.com/share?url=http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>

        </footer>



<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'dongliang'; // required: replace example with your forum shortname
    var disqus_identifier = '9';
 
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



    </article>

</div>

<div id="sidebar"><div class="box sidebox about">
  <div class="sidebox-title">About</div>
  <div class="sidebox-content">
    <p>Thoughts, stories and ideas.
记录点滴灵感...</p>

    <!--Change this to your homepage-->
    <p>See my profile at <a href="https://dongl.github.io">Dong Liang's profile</a></p>

  </div>
    
  <div class="sidebox-title">My profile</div>
  <div class="sidebox-content">
    <p>Stay tuned</p>


  </div>        
    
</div>

<!--Change the links to your sites-->
<div class="sidebox box social clearfix">
  <ul>
    <a href="../../../../rss.4" target="_blank" class="social-item rss"><li><i class="fa fa-rss"></i></li></a>
    
    <a href="http://twitter.com/ldifs" target="_blank" class="social-item tw"><li><i class="fa fa-twitter"></i></li></a>
    
    <a href="http://github.com/dongl" target="_blank" class="social-item github"><li><i class="fa fa-github"></i></li></a>
    
    <a href="http://linkedin.com" target="_blank" class="social-item home"><li><i class="fa fa-home"></i></li></a>

  </ul>

</div>

<div class="box sidebox latest-articles">
  <div class="sidebox-title">Latest Articles</div>
  <div class="sidebox-content loading">
    <div class="loader">
      <i class="fa fa-spinner fa-spin"></i>
    </div>
  </div>
</div>

</div>

</main>



    <footer class="site-footer clearfix">
         <section class="copyright"><a href="../../../../">Dong Liang's Blog</a> © 2017</section>
         <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>

    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/github.min.css">
<script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

    <script type="text/javascript" src="../../../../assets/js/jquery.min.js?v=0c292fd43d"></script>
    <script type="text/javascript" src="../../../../assets/js/jquery.fitvids.js?v=0c292fd43d"></script>
    <script type="text/javascript" src="../../../../assets/js/jquery.xml2json.js?v=0c292fd43d"></script>
    <script type="text/javascript" src="../../../../assets/js/index.js?v=0c292fd43d"></script>
    <script type="text/javascript" src="../../../../assets/js/webkid.js?v=0c292fd43d"></script>

</body>
