
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Machine learning week 5 - Neural Networks: Learning</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="../../../../assets/built/screen.css?v=91b76e839f">

    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Dong Liang's Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Machine learning week 5 - Neural Networks: Learning">
    <meta property="og:description" content="Cost Function Neural network model Let's first define a few variables that we will need to use: L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes Cost function Note: the double sum simply adds">
    <meta property="og:url" content="http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/">
    <meta property="article:published_time" content="2017-03-24T07:41:30.000Z">
    <meta property="article:modified_time" content="2017-04-13T02:49:54.000Z">
    <meta property="article:tag" content="machine learning">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Machine learning week 5 - Neural Networks: Learning">
    <meta name="twitter:description" content="Cost Function Neural network model Let's first define a few variables that we will need to use: L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes Cost function Note: the double sum simply adds">
    <meta name="twitter:url" content="http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Dong Liang">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="machine learning">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Dong Liang&#x27;s Blog",
        "logo": "http://localhost:2368/ghost/img/ghosticon.jpg"
    },
    "author": {
        "@type": "Person",
        "name": "Dong Liang",
        "url": "http://localhost:2368/author/dong/",
        "sameAs": []
    },
    "headline": "Machine learning week 5 - Neural Networks: Learning",
    "url": "http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/",
    "datePublished": "2017-03-24T07:41:30.000Z",
    "dateModified": "2017-04-13T02:49:54.000Z",
    "keywords": "machine learning",
    "description": "Cost Function Neural network model Let&#x27;s first define a few variables that we will need to use: L &#x3D; total number of layers in the network sl &#x3D; number of units (not counting bias unit) in layer l K &#x3D; number of output units/classes Cost function Note: the double sum simply adds",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368"
    }
}
    </script>

    <script type="text/javascript" src="../../../../shared/ghost-url.js?v=91b76e839f"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "25d5f7346569"
});
</script>
    <meta name="generator" content="Ghost 0.11">
    <link rel="alternate" type="application/rss+xml" title="Dong Liang's Blog" href="../../../../rss/index.html">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/github.min.css">
<script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="post-template tag-machine-learning">

    <div class="site-wrapper">

        

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
                <a class="site-nav-logo" href="../../../../">Dong Liang's Blog</a>
            <ul class="nav">
    <li class="nav-home" role="presentation"><a href="../../../../">Home</a></li>
    <li class="nav-machine-learning" role="presentation"><a href="../../../../tag/machine-learning/">Machine Learning</a></li>
</ul>
    </div>
    <div class="site-nav-right">
        <div class="social-links">
        </div>
            <a class="subscribe-button" href="index.html#subscribe">Subscribe</a>
    </div>
</nav>
    </div>
</header>


<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = /2017/03/24/machine-learning-week-5-neural-networks-learning-2/;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = ghost-; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://blogdotghost.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post tag-machine-learning featured no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="2017-03-24">24 March 2017</time>
                </section>
                <h1 class="post-full-title">Machine learning week 5 - Neural Networks: Learning</h1>
            </header>


            <section class="post-full-content">
                <h3 id="costfunction">Cost Function</h3>

<p><strong>Neural network model</strong>
<img src="http://7d9rd6.com1.z0.glb.clouddn.com/wp-content/uploads/2016/04/scrn20160408215047.png" alt="neural network"></p>

<p>Let's first define a few variables that we will need to use:</p>

<ul>
<li>L = total number of layers in the network</li>
<li>sl = number of units (not counting bias unit) in layer l</li>
<li>K = number of output units/classes</li>
</ul>

<p><mark><strong>Cost function</strong></mark></p>

<p><img src="../../../../content/images/2017/03/Screen-Shot-2017-03-24-at-1.54.34-AM.png" alt="cost function"></p>

<p><strong><em>Note:</em></strong></p>

<ul>
<li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer</li>
<li>the triple sum simply adds up the squares of all the individual Θs in the entire network.</li>
<li>the i in the triple sum does not refer to training example i</li>
</ul>

<hr>

<h3 id="backpropagationalgorithm">Backpropagation Algorithm</h3>

<blockquote>
  <p>"Backpropagation" is neural-network terminology for minimizing our cost function. The goal of Backpropagation is to minimize J(theta) - the cost function of NN, using an optimal set of parameters in theta: namely minΘJ(Θ).</p>
</blockquote>

<p><strong>Backpropagation Intuition</strong></p>

<p><img src="../../../../content/images/2017/03/Screen-Shot-2017-03-31-at-3.35.53-PM.png" alt=""></p>

<p><em>*The δ(l)j is the error for a(l)j, expressed as the derivative of the cost function.The derivative is the slope of a line tangent to the cost fucntion, so the steeper the slope the more incorrect we are. *</em></p>

<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/qc309rdcEea4MxKdJPaTxA_324034f1a3c3a3be8e7c6cfca90d3445_fixx.png?expiry=1491091200000&amp;hmac=xP_kro6Z8DjXgkDS6Fldh99xn4GgBdcP0ffsNj7UUMs" alt=""></p>

<p>In the image above, to calculate δ(2)2, we multiply the weights Θ(2)12 and Θ(2)22 by their respective δ values found to the right of each edge. So we get δ(2)2= Θ(2)12<em>δ(3)1+Θ(2)22</em>δ(3)2. To calculate every single possible δ(l)j, we could start from the right of our diagram. We can think of our edges as our Θij. Going from right to left, to calculate the value of δ(l)j, you can just take the over all sum of each weight times the δ it is coming from. Hence, another example would be δ(3)2=Θ(3)12*δ(4)1.</p>

<hr>

<h3 id="unrollingparameters">Unrolling Parameters</h3>

<ul>
<li>It becomes more convenient when the theta or gradient parameters are packed into a bigger matrix, facilitating the vectorized operations. </li>
<li>Unrolling matrix of parameters and putting them into one long vector before using the optimizing function such as fminunc(). </li>
</ul>

<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/kdK7ubT2EeajLxLfjQiSjg_d35545b8d6b6940e8577b5a8d75c8657_Screenshot-2016-11-27-15.09.24.png?expiry=1491004800000&amp;hmac=qh2Gn1mRQ-khB5O2476dP0dkmXPypNn46ODoxmTmvHM" alt=""></p>

<hr>

<h3 id="gradientchecking">Gradient Checking</h3>

<p>Gradient Checking is designed to ensure that the deltaVector function works as intended. We can approximate the derivative of the cost function J(theta). </p>

<p>The approximation can be implemented by adding or subtracting epsilon to the theta matrix, as shown in octave codes as follows:</p>

<pre><code>epsilon = 1e-4;  
for i = 1:n,  
  thetaPlus = theta;
  thetaPlus(i) += epsilon;
  thetaMinus = theta;
  thetaMinus(i) -= epsilon;
  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)
end;  
</code></pre>

<hr>

<h3 id="randominitialization">Random Initialization</h3>

<p>The use of all-zero matrix for parameters initialization would result in repeated updating to the same values in all nodes during backpropagation. Random initialization of weights for theta matrix circumvent the problem of symmetry breaking.</p>

<p>Initializing weights with zeros for theta matrix doesn't work with neural network. When we backpropogate, the optimal values will update to the same values for all nodes. </p>

<p>Briefly, we initialize each theta with a random value between [-epsilon , epsilon]. Using this formula guarantees that we get the desired bound. We can apply the same technique to multi-dimentional J(theta) matrix. </p>

<pre><code>If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  
Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  
Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  
</code></pre>

<hr>

<h3 id="puttingittogether">Putting it Together</h3>

<p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.</p>

<ul>
<li>Number of input units = dimension of features x(i)</li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</li>
<li>Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.</li>
</ul>

<p><strong>Training a Neural Network</strong></p>

<ol>
<li>Randomly initialize the weights  </li>
<li>Implement forward propagation to get hΘ(x(i)) for any x(i)  </li>
<li>Implement the cost function  </li>
<li>Implement backpropagation to compute partial derivatives  </li>
<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.  </li>
<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</li>
</ol>

<pre><code>for i = 1:m,  
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
</code></pre>

<p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/hGk18LsaEea7TQ6MHcgMPA_8de173808f362583eb39cdd0c89ef43e_Screen-Shot-2016-12-05-at-10.40.35-AM.png?expiry=1491091200000&amp;hmac=HbE_ZEpKeYD2TltrrYVU5OxTmXQd83nuso-WkozYsoM" alt=""></p>

<p>The image shows an intuition of how gradient descent works as we are implementing our neural network. The gradient descent algorithm keeps iterating and exploring the data pool until it reaches a local minimum where hΘ(x(i)) ~= y(i). The optimization process, as the the cost function is not convex, could only result in local minimum instead getting to the global optimum, but it normally doesn't cause huge problem in practice.</p>

<hr>

<h3 id="summarization">Summarization</h3>

<ul>
<li>Cost function - J(theta)</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-3.07.59-PM-1.png" alt=""></p>

<pre><code>J = (1/m) .* sum ( sum ((-yk) .* log(h_theta)  -  (1-yk) .* log(1-h_theta) )) + ...  
    lambda ./ (2 * m)  .* (sum(sum(Theta1(:, 2:end) .* Theta1(:, 2:end))) + ...
    sum(sum(Theta2(:, 2:end) .* Theta2(:, 2:end))))
</code></pre>

<ul>
<li>Forward propagation</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-7.00.04-PM.png" alt=""></p>

<ul>
<li>Backward propagation</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-3.20.34-PM-1.png" alt=""></p>

<p>The intuition behind the backward propagation can be described as: given dataset of training sample (Xt, Yt), we first calculate the activations of the whole neural network via forward propagation, including those for neural units in each hidden layer and of output value for h(theta). We then compute the error terms that reflect how much of that node was 'responsible' for any errors in our output. Concretely, for the output layer the error terms can be directly measured by subtracting actual value from network's activations. For the hidden units, the error terms in layer L can be computed based on weighted average of the error terms of the nodes in layer L + 1, that can be expressed as an equation that involves multiplication of matrix of theta, error terms of next layer and sigmoid gradient.</p>

<p><em>Sigmoid gradient</em></p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-3.15.56-PM-1.png" alt=""></p>

<p><em>Random initialization</em></p>

<pre><code>% Randomly initialize the weights to small values
% epsilon_init = sqrt(6)/(sqrt(L_in) + sqrt(L_out))
epsilon init = 0.12;  
W = rand(L out, 1 + L in) * 2 * epsilon init − epsilon init;  
</code></pre>

<ul>
<li>Backward propagation implementation</li>
</ul>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.49.30-PM.png" alt=""></p>

<p>1) Use loop function over training samples and compute the activations for each layer, namely values of z and a. Note that an all-one term needs be added to the vectors of activations to include the bias unit in each layer. </p>

<p>2) Calculate the delta value(errors) for output layer by measuring the difference between the activation and true value. </p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.37.16-PM.png" alt=""></p>

<p>3) Calculate the error terms, namely the delta value, for hidden layer as follows:</p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.40.31-PM.png" alt=""></p>

<p>4) Compute the accumulated gradient for given sample using the following formulas, where the delta2_0 is skipped or removed. </p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.45.06-PM.png" alt=""></p>

<p>5) The gradient for the neural network cost function can then be obtained by dividing the accumulated gradients by 1/m:</p>

<p><img src="../../../../content/images/2017/04/Screen-Shot-2017-04-12-at-9.46.51-PM.png" alt=""></p>
            </section>

            <section class="subscribe-form">
                <h3 class="subscribe-form-title">Subscribe to Dong Liang's Blog</h3>
                <p>Get the latest posts delivered right to your inbox</p>
                <form method="post" action="http://localhost:2368/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"><input class="location" type="hidden" name="location"><input class="referrer" type="hidden" name="referrer">

    <div class="form-group">
        <input class="subscribe-email" type="email" name="email" placeholder="youremail@example.com">
    </div>
    <button class="" type="submit">Subscribe</button>
    <script type="text/javascript">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>
</form>


            </section>

            <footer class="post-full-footer">

                <section class="author-card">
                    <section class="author-card-content">
                        <h4 class="author-card-name"><a href="../../../../author/dong/">Dong Liang</a></h4>
                            <p>Read <a href="../../../../author/dong/">more posts</a> by this author.</p>
                    </section>
                </section>
                <div class="post-full-footer-right">
                    <a class="author-card-button" href="../../../../author/dong/">Read More</a>
                </div>

            </footer>


        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">


                <article class="post-card post tag-machine-learning no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../machine-learning-week-3-logistic-regression-2/">
            <header class="post-card-header">
                <h2 class="post-card-title">Machine learning week 3 - Logistic Regression</h2>
            </header>
            <section class="post-card-excerpt">
                <p>Summarize the lesson (Logistic Regression) Logistic regression is an updated version of linear regression featuring constraint prediction values with the range of 0 to 1. The use of sigmoid or logistic function to</p>
            </section>
        </a>
        <footer class="post-card-meta">
            <span class="post-card-author"><a href="../../../../author/dong/">Dong Liang</a></span>
        </footer>
    </div>
</article>

                <article class="post-card post tag-du-shu-bi-ji no-image">
    <div class="post-card-content">
        <a class="post-card-content-link" href="../../22/bayes-factors/">
            <header class="post-card-header">
                <h2 class="post-card-title">Bayes factors</h2>
            </header>
            <section class="post-card-excerpt">
                <p>Quoted from Wikipedia   In statistics, the use of Bayes factors is a Bayesian alternative to classical hypothesis testing.[1][2] Bayesian model comparison is a method of model selection based on Bayes factors.</p>
            </section>
        </a>
        <footer class="post-card-meta">
            <span class="post-card-author"><a href="../../../../author/dong/">Dong Liang</a></span>
        </footer>
    </div>
</article>

        </div>
    </div>
</aside>

<div class="floating-header">
    <div class="floating-header-logo">
        <a href="../../../../">
            <span>Dong Liang's Blog</span>
        </a>
    </div>
    <span class="floating-header-divider">—</span>
    <div class="floating-header-title">Machine learning week 5 - Neural Networks: Learning</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"></path>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Machine%20learning%20week%205%20-%20Neural%20Networks%3A%20Learning&amp;url=http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/" onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"></path></svg>
        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/2017/03/24/machine-learning-week-5-neural-networks-learning-2/" onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"></path></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../../../../">Dong Liang's Blog</a> © 2017</section>
                <nav class="site-footer-nav">
                    <a href="../../../../">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="index.html#"></a>
        <div class="subscribe-overlay-content">
            <h1 class="subscribe-overlay-title">Subscribe to Dong Liang's Blog</h1>
            <p class="subscribe-overlay-description">Stay up to date! Get all the latest &amp; greatest posts delivered straight to your inbox</p>
            <form method="post" action="http://localhost:2368/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"><input class="location" type="hidden" name="location"><input class="referrer" type="hidden" name="referrer">

    <div class="form-group">
        <input class="subscribe-email" type="email" name="email" placeholder="youremail@example.com">
    </div>
    <button class="" type="submit">Subscribe</button>
    <script type="text/javascript">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>
</form>


        </div>
    </div>

    <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="../../../../assets/js/jquery.fitvids.js?v=91b76e839f"></script>


    <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>


    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/github.min.css">
<script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>
